{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXq5Gas5JdZl5VaJcS+kJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Ikhwan-Fathulloh/Generative-Adversarial-Network-GAN/blob/main/aae_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# Normalize the data to [0, 1] range\n",
        "mnist.data = mnist.data.float() / 255.0\n",
        "\n",
        "# Parameters\n",
        "mb_size = 32\n",
        "z_dim = 5\n",
        "X_dim = mnist.data.size(1) * mnist.data.size(2)  # Flattened image dimensions\n",
        "h_dim = 128\n",
        "lr = 1e-3\n",
        "\n",
        "# Encoder\n",
        "Q = nn.Sequential(\n",
        "    nn.Linear(X_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, z_dim)\n",
        ")\n",
        "\n",
        "# Decoder\n",
        "P = nn.Sequential(\n",
        "    nn.Linear(z_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, X_dim),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Discriminator\n",
        "D = nn.Sequential(\n",
        "    nn.Linear(z_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "def reset_grad():\n",
        "    Q.zero_grad()\n",
        "    P.zero_grad()\n",
        "    D.zero_grad()\n",
        "\n",
        "def sample_X(size):\n",
        "    indices = np.random.randint(0, len(mnist), size)\n",
        "    X = mnist.data[indices].view(size, -1).float()\n",
        "    return Variable(X)\n",
        "\n",
        "Q_solver = optim.Adam(Q.parameters(), lr=lr)\n",
        "P_solver = optim.Adam(P.parameters(), lr=lr)\n",
        "D_solver = optim.Adam(D.parameters(), lr=lr)\n",
        "\n",
        "\"\"\"1000000\"\"\"\n",
        "for it in range(100000):\n",
        "    X = sample_X(mb_size)\n",
        "\n",
        "    \"\"\" Reconstruction phase \"\"\"\n",
        "    z_sample = Q(X)\n",
        "    X_sample = P(z_sample)\n",
        "\n",
        "    # Clip values to be within [0, 1]\n",
        "    X_sample = X_sample.clamp(0, 1)\n",
        "\n",
        "    # Use BCELoss for binary cross entropy\n",
        "    recon_loss = nn.BCELoss()(X_sample, X)\n",
        "\n",
        "    recon_loss.backward()\n",
        "    P_solver.step()\n",
        "    Q_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    \"\"\" Regularization phase \"\"\"\n",
        "    # Discriminator\n",
        "    z_real = Variable(torch.randn(mb_size, z_dim))\n",
        "    z_fake = Q(X)\n",
        "\n",
        "    D_real = D(z_real)\n",
        "    D_fake = D(z_fake)\n",
        "\n",
        "    D_loss = -torch.mean(torch.log(D_real) + torch.log(1 - D_fake))\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator\n",
        "    z_fake = Q(X)\n",
        "    D_fake = D(z_fake)\n",
        "\n",
        "    G_loss = -torch.mean(torch.log(D_fake))\n",
        "\n",
        "    G_loss.backward()\n",
        "    Q_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'\n",
        "              .format(it, D_loss.item(), G_loss.item(), recon_loss.item()))\n",
        "\n",
        "        samples = P(z_real).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'\n",
        "                    .format(str(cnt).zfill(3)), bbox_inches='tight')\n",
        "        cnt += 1\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rbeetERQ76X",
        "outputId": "2b66883a-4548-40e1-c2ec-60075e2f338b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 1.463; G_loss: 0.63; recon_loss: 0.6939\n",
            "Iter-1000; D_loss: 1.249; G_loss: 1.155; recon_loss: 0.2495\n",
            "Iter-2000; D_loss: 1.38; G_loss: 0.7676; recon_loss: 0.2489\n",
            "Iter-3000; D_loss: 1.411; G_loss: 0.7568; recon_loss: 0.2443\n",
            "Iter-4000; D_loss: 1.393; G_loss: 0.6807; recon_loss: 0.2113\n",
            "Iter-5000; D_loss: 1.387; G_loss: 0.6941; recon_loss: 0.1919\n",
            "Iter-6000; D_loss: 1.382; G_loss: 0.6966; recon_loss: 0.1763\n",
            "Iter-7000; D_loss: 1.384; G_loss: 0.6923; recon_loss: 0.1795\n",
            "Iter-8000; D_loss: 1.384; G_loss: 0.7098; recon_loss: 0.1721\n",
            "Iter-9000; D_loss: 1.387; G_loss: 0.6938; recon_loss: 0.171\n",
            "Iter-10000; D_loss: 1.371; G_loss: 0.7022; recon_loss: 0.1612\n",
            "Iter-11000; D_loss: 1.391; G_loss: 0.6997; recon_loss: 0.1737\n",
            "Iter-12000; D_loss: 1.413; G_loss: 0.6898; recon_loss: 0.1663\n",
            "Iter-13000; D_loss: 1.389; G_loss: 0.7057; recon_loss: 0.1587\n",
            "Iter-14000; D_loss: 1.385; G_loss: 0.6926; recon_loss: 0.1565\n",
            "Iter-15000; D_loss: 1.396; G_loss: 0.6843; recon_loss: 0.1717\n",
            "Iter-16000; D_loss: 1.39; G_loss: 0.7006; recon_loss: 0.1575\n",
            "Iter-17000; D_loss: 1.386; G_loss: 0.6904; recon_loss: 0.1604\n",
            "Iter-18000; D_loss: 1.386; G_loss: 0.6853; recon_loss: 0.1491\n",
            "Iter-19000; D_loss: 1.402; G_loss: 0.6797; recon_loss: 0.1449\n",
            "Iter-20000; D_loss: 1.384; G_loss: 0.6817; recon_loss: 0.1712\n",
            "Iter-21000; D_loss: 1.388; G_loss: 0.6893; recon_loss: 0.1566\n",
            "Iter-22000; D_loss: 1.412; G_loss: 0.6817; recon_loss: 0.1572\n",
            "Iter-23000; D_loss: 1.383; G_loss: 0.6903; recon_loss: 0.1609\n",
            "Iter-24000; D_loss: 1.399; G_loss: 0.6911; recon_loss: 0.1542\n",
            "Iter-25000; D_loss: 1.381; G_loss: 0.6982; recon_loss: 0.167\n",
            "Iter-26000; D_loss: 1.38; G_loss: 0.6795; recon_loss: 0.1542\n",
            "Iter-27000; D_loss: 1.395; G_loss: 0.6815; recon_loss: 0.1638\n",
            "Iter-28000; D_loss: 1.386; G_loss: 0.6939; recon_loss: 0.1696\n",
            "Iter-29000; D_loss: 1.406; G_loss: 0.6933; recon_loss: 0.1764\n",
            "Iter-30000; D_loss: 1.38; G_loss: 0.6843; recon_loss: 0.1561\n",
            "Iter-31000; D_loss: 1.386; G_loss: 0.6993; recon_loss: 0.1505\n",
            "Iter-32000; D_loss: 1.379; G_loss: 0.6981; recon_loss: 0.1645\n",
            "Iter-33000; D_loss: 1.392; G_loss: 0.6867; recon_loss: 0.1609\n",
            "Iter-34000; D_loss: 1.399; G_loss: 0.6802; recon_loss: 0.1648\n",
            "Iter-35000; D_loss: 1.387; G_loss: 0.7054; recon_loss: 0.1754\n",
            "Iter-36000; D_loss: 1.391; G_loss: 0.6837; recon_loss: 0.1508\n",
            "Iter-37000; D_loss: 1.373; G_loss: 0.6927; recon_loss: 0.1677\n",
            "Iter-38000; D_loss: 1.385; G_loss: 0.6891; recon_loss: 0.1624\n",
            "Iter-39000; D_loss: 1.394; G_loss: 0.6841; recon_loss: 0.1581\n",
            "Iter-40000; D_loss: 1.386; G_loss: 0.6907; recon_loss: 0.1497\n",
            "Iter-41000; D_loss: 1.384; G_loss: 0.693; recon_loss: 0.1477\n",
            "Iter-42000; D_loss: 1.372; G_loss: 0.6999; recon_loss: 0.1718\n",
            "Iter-43000; D_loss: 1.377; G_loss: 0.7; recon_loss: 0.1578\n",
            "Iter-44000; D_loss: 1.387; G_loss: 0.6876; recon_loss: 0.1591\n",
            "Iter-45000; D_loss: 1.389; G_loss: 0.7019; recon_loss: 0.1545\n",
            "Iter-46000; D_loss: 1.381; G_loss: 0.69; recon_loss: 0.1594\n",
            "Iter-47000; D_loss: 1.4; G_loss: 0.6948; recon_loss: 0.1547\n",
            "Iter-48000; D_loss: 1.385; G_loss: 0.6911; recon_loss: 0.1527\n",
            "Iter-49000; D_loss: 1.395; G_loss: 0.6762; recon_loss: 0.168\n",
            "Iter-50000; D_loss: 1.4; G_loss: 0.6922; recon_loss: 0.1479\n",
            "Iter-51000; D_loss: 1.384; G_loss: 0.6947; recon_loss: 0.1558\n",
            "Iter-52000; D_loss: 1.38; G_loss: 0.6872; recon_loss: 0.1494\n",
            "Iter-53000; D_loss: 1.38; G_loss: 0.7069; recon_loss: 0.1496\n",
            "Iter-54000; D_loss: 1.376; G_loss: 0.6955; recon_loss: 0.1508\n",
            "Iter-55000; D_loss: 1.383; G_loss: 0.6949; recon_loss: 0.1449\n",
            "Iter-56000; D_loss: 1.387; G_loss: 0.7147; recon_loss: 0.1642\n",
            "Iter-57000; D_loss: 1.394; G_loss: 0.685; recon_loss: 0.1533\n",
            "Iter-58000; D_loss: 1.383; G_loss: 0.684; recon_loss: 0.1661\n",
            "Iter-59000; D_loss: 1.389; G_loss: 0.6854; recon_loss: 0.1763\n",
            "Iter-60000; D_loss: 1.392; G_loss: 0.6919; recon_loss: 0.1578\n",
            "Iter-61000; D_loss: 1.391; G_loss: 0.699; recon_loss: 0.1523\n",
            "Iter-62000; D_loss: 1.41; G_loss: 0.6781; recon_loss: 0.163\n",
            "Iter-63000; D_loss: 1.384; G_loss: 0.6882; recon_loss: 0.1606\n",
            "Iter-64000; D_loss: 1.388; G_loss: 0.6896; recon_loss: 0.137\n",
            "Iter-65000; D_loss: 1.365; G_loss: 0.698; recon_loss: 0.1636\n",
            "Iter-66000; D_loss: 1.387; G_loss: 0.6857; recon_loss: 0.1539\n",
            "Iter-67000; D_loss: 1.389; G_loss: 0.692; recon_loss: 0.1671\n",
            "Iter-68000; D_loss: 1.394; G_loss: 0.6973; recon_loss: 0.1543\n",
            "Iter-69000; D_loss: 1.389; G_loss: 0.7113; recon_loss: 0.1586\n",
            "Iter-70000; D_loss: 1.384; G_loss: 0.686; recon_loss: 0.1563\n",
            "Iter-71000; D_loss: 1.385; G_loss: 0.697; recon_loss: 0.1675\n",
            "Iter-72000; D_loss: 1.381; G_loss: 0.6907; recon_loss: 0.1567\n",
            "Iter-73000; D_loss: 1.404; G_loss: 0.6835; recon_loss: 0.1476\n",
            "Iter-74000; D_loss: 1.391; G_loss: 0.6933; recon_loss: 0.1437\n",
            "Iter-75000; D_loss: 1.389; G_loss: 0.6843; recon_loss: 0.1641\n",
            "Iter-76000; D_loss: 1.387; G_loss: 0.6959; recon_loss: 0.1442\n",
            "Iter-77000; D_loss: 1.392; G_loss: 0.6815; recon_loss: 0.1556\n",
            "Iter-78000; D_loss: 1.389; G_loss: 0.6974; recon_loss: 0.1542\n",
            "Iter-79000; D_loss: 1.384; G_loss: 0.6977; recon_loss: 0.16\n",
            "Iter-80000; D_loss: 1.398; G_loss: 0.6851; recon_loss: 0.1555\n",
            "Iter-81000; D_loss: 1.379; G_loss: 0.7031; recon_loss: 0.1543\n",
            "Iter-82000; D_loss: 1.379; G_loss: 0.6844; recon_loss: 0.1646\n",
            "Iter-83000; D_loss: 1.394; G_loss: 0.678; recon_loss: 0.1637\n",
            "Iter-84000; D_loss: 1.375; G_loss: 0.6924; recon_loss: 0.1656\n",
            "Iter-85000; D_loss: 1.395; G_loss: 0.6871; recon_loss: 0.1604\n",
            "Iter-86000; D_loss: 1.38; G_loss: 0.7051; recon_loss: 0.1617\n",
            "Iter-87000; D_loss: 1.381; G_loss: 0.702; recon_loss: 0.1446\n",
            "Iter-88000; D_loss: 1.381; G_loss: 0.6969; recon_loss: 0.1674\n",
            "Iter-89000; D_loss: 1.379; G_loss: 0.6988; recon_loss: 0.1616\n",
            "Iter-90000; D_loss: 1.4; G_loss: 0.6833; recon_loss: 0.1481\n",
            "Iter-91000; D_loss: 1.397; G_loss: 0.688; recon_loss: 0.169\n",
            "Iter-92000; D_loss: 1.388; G_loss: 0.6999; recon_loss: 0.1452\n",
            "Iter-93000; D_loss: 1.386; G_loss: 0.6893; recon_loss: 0.1725\n",
            "Iter-94000; D_loss: 1.385; G_loss: 0.6936; recon_loss: 0.1595\n",
            "Iter-95000; D_loss: 1.384; G_loss: 0.6884; recon_loss: 0.1633\n",
            "Iter-96000; D_loss: 1.364; G_loss: 0.7381; recon_loss: 0.1595\n",
            "Iter-97000; D_loss: 1.383; G_loss: 0.7033; recon_loss: 0.1572\n",
            "Iter-98000; D_loss: 1.386; G_loss: 0.687; recon_loss: 0.1511\n",
            "Iter-99000; D_loss: 1.388; G_loss: 0.6851; recon_loss: 0.1507\n"
          ]
        }
      ]
    }
  ]
}