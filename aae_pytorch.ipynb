{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd7DOLvsdVoWuIkd6s/ZPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Ikhwan-Fathulloh/Generative-Adversarial-Network-GAN/blob/main/aae_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# Normalize the data to [0, 1] range\n",
        "mnist.data = mnist.data.float() / 255.0\n",
        "\n",
        "# Parameters\n",
        "mb_size = 32\n",
        "z_dim = 5\n",
        "X_dim = mnist.data.size(1) * mnist.data.size(2)  # Flattened image dimensions\n",
        "h_dim = 128\n",
        "lr = 1e-3\n",
        "\n",
        "# Encoder\n",
        "Q = nn.Sequential(\n",
        "    nn.Linear(X_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, z_dim)\n",
        ")\n",
        "\n",
        "# Decoder\n",
        "P = nn.Sequential(\n",
        "    nn.Linear(z_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, X_dim),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Discriminator\n",
        "D = nn.Sequential(\n",
        "    nn.Linear(z_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "def reset_grad():\n",
        "    Q.zero_grad()\n",
        "    P.zero_grad()\n",
        "    D.zero_grad()\n",
        "\n",
        "def sample_X(size):\n",
        "    indices = np.random.randint(0, len(mnist), size)\n",
        "    X = mnist.data[indices].view(size, -1).float()\n",
        "    return Variable(X)\n",
        "\n",
        "Q_solver = optim.Adam(Q.parameters(), lr=lr)\n",
        "P_solver = optim.Adam(P.parameters(), lr=lr)\n",
        "D_solver = optim.Adam(D.parameters(), lr=lr)\n",
        "cnt = 0\n",
        "\n",
        "\"\"\"1000000\"\"\"\n",
        "for it in range(100000):\n",
        "    X = sample_X(mb_size)\n",
        "\n",
        "    \"\"\" Reconstruction phase \"\"\"\n",
        "    z_sample = Q(X)\n",
        "    X_sample = P(z_sample)\n",
        "\n",
        "    # Clip values to be within [0, 1]\n",
        "    X_sample = X_sample.clamp(0, 1)\n",
        "\n",
        "    # Use BCELoss for binary cross entropy\n",
        "    recon_loss = nn.BCELoss()(X_sample, X)\n",
        "\n",
        "    recon_loss.backward()\n",
        "    P_solver.step()\n",
        "    Q_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    \"\"\" Regularization phase \"\"\"\n",
        "    # Discriminator\n",
        "    z_real = Variable(torch.randn(mb_size, z_dim))\n",
        "    z_fake = Q(X)\n",
        "\n",
        "    D_real = D(z_real)\n",
        "    D_fake = D(z_fake)\n",
        "\n",
        "    D_loss = -torch.mean(torch.log(D_real) + torch.log(1 - D_fake))\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator\n",
        "    z_fake = Q(X)\n",
        "    D_fake = D(z_fake)\n",
        "\n",
        "    G_loss = -torch.mean(torch.log(D_fake))\n",
        "\n",
        "    G_loss.backward()\n",
        "    Q_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'\n",
        "              .format(it, D_loss.item(), G_loss.item(), recon_loss.item()))\n",
        "\n",
        "        samples = P(z_real).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'\n",
        "                    .format(str(cnt).zfill(3)), bbox_inches='tight')\n",
        "        cnt += 1\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rbeetERQ76X",
        "outputId": "f157e46f-84c2-4c36-a550-108744e81d48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 1.412; G_loss: 0.692; recon_loss: 0.6968\n",
            "Iter-1000; D_loss: 1.343; G_loss: 0.8248; recon_loss: 0.2599\n",
            "Iter-2000; D_loss: 1.671; G_loss: 0.4926; recon_loss: 0.2348\n",
            "Iter-3000; D_loss: 1.445; G_loss: 0.6041; recon_loss: 0.219\n",
            "Iter-4000; D_loss: 1.402; G_loss: 0.6601; recon_loss: 0.2265\n",
            "Iter-5000; D_loss: 1.384; G_loss: 0.7129; recon_loss: 0.182\n",
            "Iter-6000; D_loss: 1.384; G_loss: 0.6986; recon_loss: 0.1655\n",
            "Iter-7000; D_loss: 1.389; G_loss: 0.6934; recon_loss: 0.1678\n",
            "Iter-8000; D_loss: 1.379; G_loss: 0.7163; recon_loss: 0.1691\n",
            "Iter-9000; D_loss: 1.383; G_loss: 0.686; recon_loss: 0.1895\n",
            "Iter-10000; D_loss: 1.389; G_loss: 0.6803; recon_loss: 0.1675\n",
            "Iter-11000; D_loss: 1.389; G_loss: 0.6995; recon_loss: 0.1781\n",
            "Iter-12000; D_loss: 1.389; G_loss: 0.6828; recon_loss: 0.1704\n",
            "Iter-13000; D_loss: 1.391; G_loss: 0.6991; recon_loss: 0.1466\n",
            "Iter-14000; D_loss: 1.392; G_loss: 0.69; recon_loss: 0.1744\n",
            "Iter-15000; D_loss: 1.373; G_loss: 0.6861; recon_loss: 0.1545\n",
            "Iter-16000; D_loss: 1.383; G_loss: 0.689; recon_loss: 0.1538\n",
            "Iter-17000; D_loss: 1.379; G_loss: 0.6996; recon_loss: 0.1568\n",
            "Iter-18000; D_loss: 1.392; G_loss: 0.6886; recon_loss: 0.1678\n",
            "Iter-19000; D_loss: 1.376; G_loss: 0.6967; recon_loss: 0.1327\n",
            "Iter-20000; D_loss: 1.392; G_loss: 0.6981; recon_loss: 0.1597\n",
            "Iter-21000; D_loss: 1.377; G_loss: 0.7049; recon_loss: 0.1623\n",
            "Iter-22000; D_loss: 1.384; G_loss: 0.6949; recon_loss: 0.1647\n",
            "Iter-23000; D_loss: 1.39; G_loss: 0.7074; recon_loss: 0.1776\n",
            "Iter-24000; D_loss: 1.388; G_loss: 0.7014; recon_loss: 0.1789\n",
            "Iter-25000; D_loss: 1.389; G_loss: 0.6856; recon_loss: 0.1673\n",
            "Iter-26000; D_loss: 1.383; G_loss: 0.6923; recon_loss: 0.1601\n",
            "Iter-27000; D_loss: 1.386; G_loss: 0.689; recon_loss: 0.1356\n",
            "Iter-28000; D_loss: 1.383; G_loss: 0.6868; recon_loss: 0.1498\n",
            "Iter-29000; D_loss: 1.384; G_loss: 0.687; recon_loss: 0.1556\n",
            "Iter-30000; D_loss: 1.391; G_loss: 0.7012; recon_loss: 0.1629\n",
            "Iter-31000; D_loss: 1.396; G_loss: 0.6892; recon_loss: 0.1616\n",
            "Iter-32000; D_loss: 1.394; G_loss: 0.6869; recon_loss: 0.157\n",
            "Iter-33000; D_loss: 1.39; G_loss: 0.69; recon_loss: 0.1602\n",
            "Iter-34000; D_loss: 1.385; G_loss: 0.694; recon_loss: 0.1551\n",
            "Iter-35000; D_loss: 1.384; G_loss: 0.698; recon_loss: 0.1561\n",
            "Iter-36000; D_loss: 1.391; G_loss: 0.6913; recon_loss: 0.1656\n",
            "Iter-37000; D_loss: 1.379; G_loss: 0.691; recon_loss: 0.1457\n",
            "Iter-38000; D_loss: 1.382; G_loss: 0.6942; recon_loss: 0.1588\n",
            "Iter-39000; D_loss: 1.392; G_loss: 0.685; recon_loss: 0.1551\n",
            "Iter-40000; D_loss: 1.394; G_loss: 0.6866; recon_loss: 0.1616\n",
            "Iter-41000; D_loss: 1.383; G_loss: 0.6977; recon_loss: 0.1518\n",
            "Iter-42000; D_loss: 1.393; G_loss: 0.6877; recon_loss: 0.1534\n",
            "Iter-43000; D_loss: 1.382; G_loss: 0.6937; recon_loss: 0.154\n",
            "Iter-44000; D_loss: 1.381; G_loss: 0.6916; recon_loss: 0.1508\n",
            "Iter-45000; D_loss: 1.388; G_loss: 0.6881; recon_loss: 0.1611\n",
            "Iter-46000; D_loss: 1.391; G_loss: 0.6919; recon_loss: 0.1592\n",
            "Iter-47000; D_loss: 1.379; G_loss: 0.696; recon_loss: 0.1439\n",
            "Iter-48000; D_loss: 1.382; G_loss: 0.7001; recon_loss: 0.1535\n",
            "Iter-49000; D_loss: 1.38; G_loss: 0.6989; recon_loss: 0.1442\n",
            "Iter-50000; D_loss: 1.409; G_loss: 0.6835; recon_loss: 0.1627\n",
            "Iter-51000; D_loss: 1.386; G_loss: 0.688; recon_loss: 0.1544\n",
            "Iter-52000; D_loss: 1.394; G_loss: 0.6828; recon_loss: 0.1473\n",
            "Iter-53000; D_loss: 1.398; G_loss: 0.6804; recon_loss: 0.1396\n",
            "Iter-54000; D_loss: 1.387; G_loss: 0.6891; recon_loss: 0.1463\n",
            "Iter-55000; D_loss: 1.392; G_loss: 0.6983; recon_loss: 0.1632\n",
            "Iter-56000; D_loss: 1.39; G_loss: 0.6893; recon_loss: 0.1661\n",
            "Iter-57000; D_loss: 1.379; G_loss: 0.7022; recon_loss: 0.1746\n",
            "Iter-58000; D_loss: 1.388; G_loss: 0.6969; recon_loss: 0.1527\n",
            "Iter-59000; D_loss: 1.382; G_loss: 0.6939; recon_loss: 0.149\n",
            "Iter-60000; D_loss: 1.388; G_loss: 0.6935; recon_loss: 0.1734\n",
            "Iter-61000; D_loss: 1.386; G_loss: 0.6912; recon_loss: 0.1647\n",
            "Iter-62000; D_loss: 1.381; G_loss: 0.6925; recon_loss: 0.1697\n",
            "Iter-63000; D_loss: 1.382; G_loss: 0.7008; recon_loss: 0.1478\n",
            "Iter-64000; D_loss: 1.387; G_loss: 0.6885; recon_loss: 0.157\n",
            "Iter-65000; D_loss: 1.39; G_loss: 0.6907; recon_loss: 0.1782\n",
            "Iter-66000; D_loss: 1.386; G_loss: 0.6925; recon_loss: 0.159\n",
            "Iter-67000; D_loss: 1.385; G_loss: 0.6941; recon_loss: 0.1555\n",
            "Iter-68000; D_loss: 1.382; G_loss: 0.6909; recon_loss: 0.1511\n",
            "Iter-69000; D_loss: 1.385; G_loss: 0.6913; recon_loss: 0.1372\n",
            "Iter-70000; D_loss: 1.363; G_loss: 0.7325; recon_loss: 0.1388\n",
            "Iter-71000; D_loss: 1.386; G_loss: 0.6918; recon_loss: 0.1509\n",
            "Iter-72000; D_loss: 1.392; G_loss: 0.6877; recon_loss: 0.1519\n",
            "Iter-73000; D_loss: 1.387; G_loss: 0.6927; recon_loss: 0.1505\n",
            "Iter-74000; D_loss: 1.376; G_loss: 0.7044; recon_loss: 0.1608\n",
            "Iter-75000; D_loss: 1.382; G_loss: 0.6971; recon_loss: 0.1502\n",
            "Iter-76000; D_loss: 1.382; G_loss: 0.6902; recon_loss: 0.1717\n",
            "Iter-77000; D_loss: 1.384; G_loss: 0.6923; recon_loss: 0.153\n",
            "Iter-78000; D_loss: 1.39; G_loss: 0.69; recon_loss: 0.1445\n",
            "Iter-79000; D_loss: 1.38; G_loss: 0.6953; recon_loss: 0.1492\n",
            "Iter-80000; D_loss: 1.384; G_loss: 0.697; recon_loss: 0.1612\n",
            "Iter-81000; D_loss: 1.379; G_loss: 0.6961; recon_loss: 0.148\n",
            "Iter-82000; D_loss: 1.384; G_loss: 0.7034; recon_loss: 0.1518\n",
            "Iter-83000; D_loss: 1.388; G_loss: 0.6916; recon_loss: 0.1604\n",
            "Iter-84000; D_loss: 1.381; G_loss: 0.7007; recon_loss: 0.14\n",
            "Iter-85000; D_loss: 1.392; G_loss: 0.6956; recon_loss: 0.1766\n",
            "Iter-86000; D_loss: 1.387; G_loss: 0.6929; recon_loss: 0.1562\n",
            "Iter-87000; D_loss: 1.395; G_loss: 0.6927; recon_loss: 0.1612\n",
            "Iter-88000; D_loss: 1.389; G_loss: 0.699; recon_loss: 0.1579\n",
            "Iter-89000; D_loss: 1.387; G_loss: 0.687; recon_loss: 0.157\n",
            "Iter-90000; D_loss: 1.387; G_loss: 0.6905; recon_loss: 0.154\n",
            "Iter-91000; D_loss: 1.39; G_loss: 0.6892; recon_loss: 0.1607\n",
            "Iter-92000; D_loss: 1.385; G_loss: 0.692; recon_loss: 0.1597\n",
            "Iter-93000; D_loss: 1.386; G_loss: 0.6905; recon_loss: 0.1381\n",
            "Iter-94000; D_loss: 1.378; G_loss: 0.6904; recon_loss: 0.1457\n",
            "Iter-95000; D_loss: 1.393; G_loss: 0.6831; recon_loss: 0.1616\n",
            "Iter-96000; D_loss: 1.39; G_loss: 0.6893; recon_loss: 0.1642\n",
            "Iter-97000; D_loss: 1.396; G_loss: 0.6907; recon_loss: 0.1547\n",
            "Iter-98000; D_loss: 1.387; G_loss: 0.6871; recon_loss: 0.1417\n",
            "Iter-99000; D_loss: 1.386; G_loss: 0.6978; recon_loss: 0.1482\n"
          ]
        }
      ]
    }
  ]
}